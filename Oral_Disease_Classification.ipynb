{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnI9Flo8Mx8H",
        "outputId": "5a1b12c0-1e14-42a0-d319-e3577c619e0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/archive (13)/OA/TRAIN\"\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    validation_split=0.2\n",
        ").flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    validation_split=0.2\n",
        ").flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n",
        "    \"/content/drive/MyDrive/archive (13)/OA/TEST\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(img_height, img_width, 3)\n",
        ")\n",
        "\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(train_ds.num_classes, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_ds),\n",
        "    validation_steps=len(val_ds),\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")\n",
        "\n",
        "# Save the Model\n",
        "model.save(\"imagenet_finetuned_model.keras\")\n",
        "\n",
        "# Evaluate the Model on Test Data\n",
        "test_loss, test_accuracy = model.evaluate(test_ds, steps=len(test_ds))\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Classification Report\n",
        "y_true = test_ds.classes\n",
        "y_pred = model.predict(test_ds, steps=len(test_ds))\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred_classes, target_names=test_ds.class_indices.keys()))\n"
      ],
      "metadata": {
        "id": "h6oUEYjK8_X9",
        "outputId": "db1afd06-0055-4378-e2a1-969bfdb4e5d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1198 images belonging to 2 classes.\n",
            "Found 298 images belonging to 2 classes.\n",
            "Found 408 images belonging to 2 classes.\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 463ms/step - accuracy: 0.8112 - loss: 0.4466 - val_accuracy: 0.8221 - val_loss: 0.5220 - learning_rate: 0.0010\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 163ms/step - accuracy: 0.9617 - loss: 0.0995 - val_accuracy: 0.8087 - val_loss: 0.5560 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 161ms/step - accuracy: 0.9809 - loss: 0.0589 - val_accuracy: 0.8221 - val_loss: 0.5689 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 189ms/step - accuracy: 0.9745 - loss: 0.0657 - val_accuracy: 0.8356 - val_loss: 0.5154 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 164ms/step - accuracy: 0.9840 - loss: 0.0391 - val_accuracy: 0.8523 - val_loss: 0.5828 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 185ms/step - accuracy: 0.9911 - loss: 0.0362 - val_accuracy: 0.8423 - val_loss: 0.6055 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9887 - loss: 0.0359\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - accuracy: 0.9887 - loss: 0.0361 - val_accuracy: 0.8255 - val_loss: 0.6464 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 166ms/step - accuracy: 0.9918 - loss: 0.0296 - val_accuracy: 0.8389 - val_loss: 0.6015 - learning_rate: 5.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 169ms/step - accuracy: 0.9964 - loss: 0.0165 - val_accuracy: 0.8389 - val_loss: 0.5938 - learning_rate: 5.0000e-04\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 536ms/step - accuracy: 0.9586 - loss: 0.1369\n",
            "Test Loss: 0.16029785573482513, Test Accuracy: 0.9411764740943909\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 360ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Caries       0.91      0.99      0.94       204\n",
            "  Gingivitis       0.98      0.90      0.94       204\n",
            "\n",
            "    accuracy                           0.94       408\n",
            "   macro avg       0.94      0.94      0.94       408\n",
            "weighted avg       0.94      0.94      0.94       408\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image_path, model, class_indices):\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = load_img(image_path, target_size=(img_height, img_width))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # To Generate predictions\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    confidence_score = predictions[0][predicted_class_index] * 100  # Convert to percentage\n",
        "\n",
        "    # Map index to class label\n",
        "    class_labels = {v: k for k, v in class_indices.items()}\n",
        "    predicted_label = class_labels[predicted_class_index]\n",
        "\n",
        "    return predicted_label, confidence_score\n",
        "\n",
        "image_path = \"/content/drive/MyDrive/archive (13)/OA/TEST/Caries/202_2.jpg\"\n",
        "predicted_label, confidence_score = predict_image(image_path, model, train_ds.class_indices)\n",
        "\n",
        "print(f\"The predicted class for the image is: {predicted_label}\")\n",
        "print(f\"Confidence score: {confidence_score:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "U54b8D1hCuRH",
        "outputId": "79cbb407-98f9-416d-b2b5-9c488cd6f5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "The predicted class for the image is: Caries\n",
            "Confidence score: 99.62%\n"
          ]
        }
      ]
    }
  ]
}